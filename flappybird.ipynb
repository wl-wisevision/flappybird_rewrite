{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import wrapped_flappy_bird as game\n",
    "from collections import deque\n",
    "import cv2\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAME = 'flappybird'\n",
    "ACTIONS = 2 # numbers of valid actions\n",
    "INITIAL_EPSILON = 0.1\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "FRAME_PER_ACTION=1\n",
    "OBSERVE = 100000. # timesteps to observe before training\n",
    "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "GAMMA = 0.99 # decay rate of past observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "def max_pool(x): #2*2\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createNetwork():\n",
    "    W_conv1 = weight_variable([8, 8, 4, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    W_conv2 = weight_variable([4, 4, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    W_conv3 = weight_variable([3, 3, 64, 64])\n",
    "    b_conv3 = bias_variable([64])\n",
    "\n",
    "    W_fc1 = weight_variable([256, 256])\n",
    "    b_fc1 = bias_variable([256])\n",
    "\n",
    "    W_fc2 = weight_variable([256, ACTIONS])\n",
    "    b_fc2 = bias_variable([ACTIONS])\n",
    "    \n",
    "    #input layer\n",
    "    s = tf.placeholder('float', [None, 80, 80, 4])\n",
    "    \n",
    "    #hidden layers\n",
    "    h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1) #20*20*32\n",
    "    h_pool1 = max_pool(h_conv1) #10*10*32\n",
    "    \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2) #5*5*64\n",
    "    h_pool2 = max_pool(h_conv2) #3*3*64\n",
    "    \n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3, 1) + b_conv3) #3*3*64\n",
    "    h_pool3 = max_pool(h_conv3) #2*2*64=256\n",
    "    \n",
    "    #full connect\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [-1, 256])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    #readout layer\n",
    "    readout = tf.matmul(h_fc1, W_fc2) +b_fc2\n",
    "    \n",
    "    return s, readout, h_fc1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNetwork(s, readout, h_fc1, sess):\n",
    "    #define cost function\n",
    "    a = tf.placeholder('float', [None, ACTIONS])\n",
    "    y = tf.placeholder('float', [None])\n",
    "    readout_action = tf.reduce_sum(tf.multiply(readout, a))\n",
    "    cost = tf. reduce_mean(tf.square(y - readout_action))\n",
    "    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n",
    "    \n",
    "    #open up a game state to communicate with emulator\n",
    "    game_state = game.GameState()\n",
    "    \n",
    "    #store the previous observations in replay memory\n",
    "    D = deque()\n",
    "    \n",
    "    #get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1\n",
    "    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t, 1, 255, cv2.THRESH_BINARY)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    \n",
    "    #saving and loading networks\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoint = tf.train.get_checkpoint_state('saved_networks')\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print('successfully loaded:', checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print('could not find old network weights')\n",
    "        \n",
    "    #start training \n",
    "    epsilon = INITIAL_EPSILON\n",
    "    t = 0\n",
    "    while 'flappy bird' !='angry bird':\n",
    "        #choose an action epsilon greedily\n",
    "        readout_t = readout.eval(feed_dict={s:[s_t]})[0]\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        action_index = 0\n",
    "        if t%FRAME_PER_ACTION==0:\n",
    "            if random.random() <= epsilon:\n",
    "                print('----Random Action ----')\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[random.randrange(ACTIONS)] = 1\n",
    "            else:\n",
    "                action_index = np.argmax(readout_t)\n",
    "                a_t[action_index] = 1\n",
    "        else:\n",
    "            a_t[0] = 1 # do nothing\n",
    "        \n",
    "        #scale down epsilon\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/EXPLORE\n",
    "        \n",
    "        # run the selected action and observe next state and reward \n",
    "        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
    "        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "        ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
    "        x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
    "        \n",
    "        #store the transition in D\n",
    "        D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "            \n",
    "        #only train if done observing\n",
    "        if t > OBSERVE:\n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            \n",
    "            #get the batch variables\n",
    "            s_j_batch = [d[0] for d in minibatch]\n",
    "            a_batch = [d[1] for d in minibatch]\n",
    "            r_batch = [d[2] for d in minibatch]\n",
    "            s_j1_batch = [d[3] for d in minibatch]\n",
    "            \n",
    "            y_batch = []\n",
    "            readout_j1_batch = readout.eval(feed_dict={s: s_j1_batch})\n",
    "            for i in range(0, len(minibatch)):\n",
    "                terminal = minibatch[i][4]\n",
    "                #if terminal  , only queals reward \n",
    "                if terminal:\n",
    "                    y_batch.append(r_batch[i])\n",
    "                else:\n",
    "                    y_batch.append(r_batch[i] + GAMMA*np.max(readout_j1_batch[i]))\n",
    "                    \n",
    "            #perform gradient step\n",
    "            train_step.run(feed_dict={y:y_batch,\n",
    "                                     a:a_batch,\n",
    "                                     s:s_j_batch})\n",
    "        \n",
    "        #update the old values\n",
    "        s_t = s_t1\n",
    "        t += 1\n",
    "        \n",
    "        #save progress every 10000 iterations\n",
    "        if t%10000 ==0:\n",
    "            saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)\n",
    "        \n",
    "        #print info \n",
    "        state = ''\n",
    "        if t<=OBSERVE:\n",
    "            state = 'observe'\n",
    "        elif t>OBSERVE and t<=OBSERVE + EXPLORE:\n",
    "            state = 'explore'\n",
    "        else:\n",
    "            state = 'train'\n",
    "            \n",
    "        print('timestep', t, '/state', state, '/epsilon', epsilon, \n",
    "              '/action', action_index, '/reward', r_t, '/q_max', np.max(readout_t))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    sess = tf.InteractiveSession()\n",
    "    s, readout, h_fc1 = createNetwork()\n",
    "    trainNetwork(s, readout, h_fc1, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_networks/flappybird-dqn-1390000\n",
      "successfully loaded: saved_networks/flappybird-dqn-1390000\n",
      "timestep 1 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152985\n",
      "----Random Action ----\n",
      "timestep 2 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.014329\n",
      "timestep 3 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148235\n",
      "----Random Action ----\n",
      "timestep 4 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149867\n",
      "timestep 5 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146487\n",
      "timestep 6 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152616\n",
      "timestep 7 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159049\n",
      "----Random Action ----\n",
      "timestep 8 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016065\n",
      "timestep 9 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162853\n",
      "timestep 10 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163233\n",
      "timestep 11 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147756\n",
      "timestep 12 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151543\n",
      "timestep 13 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0142448\n",
      "timestep 14 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163298\n",
      "----Random Action ----\n",
      "timestep 15 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158854\n",
      "timestep 16 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.017067\n",
      "timestep 17 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155791\n",
      "timestep 18 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150839\n",
      "timestep 19 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166746\n",
      "timestep 20 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175059\n",
      "timestep 21 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159186\n",
      "timestep 22 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152997\n",
      "timestep 23 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156989\n",
      "timestep 24 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141519\n",
      "timestep 25 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0129105\n",
      "timestep 26 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0131591\n",
      "timestep 27 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0130399\n",
      "timestep 28 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0132218\n",
      "timestep 29 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0140517\n",
      "timestep 30 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147569\n",
      "timestep 31 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153803\n",
      "timestep 32 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015777\n",
      "timestep 33 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175606\n",
      "timestep 34 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179892\n",
      "timestep 35 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0174432\n",
      "----Random Action ----\n",
      "timestep 36 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160617\n",
      "timestep 37 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0185279\n",
      "timestep 38 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0178273\n",
      "timestep 39 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170226\n",
      "timestep 40 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015279\n",
      "timestep 41 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169826\n",
      "timestep 42 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0171653\n",
      "timestep 43 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176831\n",
      "timestep 44 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015936\n",
      "----Random Action ----\n",
      "timestep 45 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168577\n",
      "timestep 46 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176353\n",
      "timestep 47 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173687\n",
      "timestep 48 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0182907\n",
      "timestep 49 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0149135\n",
      "timestep 50 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0219278\n",
      "----Random Action ----\n",
      "timestep 51 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0172932\n",
      "timestep 52 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158651\n",
      "timestep 53 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149111\n",
      "timestep 54 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141245\n",
      "timestep 55 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0143289\n",
      "timestep 56 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154264\n",
      "timestep 57 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156791\n",
      "timestep 58 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163016\n",
      "timestep 59 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158934\n",
      "timestep 60 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151973\n",
      "timestep 61 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152123\n",
      "timestep 62 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014753\n",
      "timestep 63 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0144953\n",
      "timestep 64 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154515\n",
      "timestep 65 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156551\n",
      "timestep 66 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0161589\n",
      "timestep 67 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150687\n",
      "timestep 68 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155033\n",
      "timestep 69 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150134\n",
      "timestep 70 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172856\n",
      "timestep 71 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153374\n",
      "timestep 72 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154938\n",
      "timestep 73 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147382\n",
      "timestep 74 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152754\n",
      "timestep 75 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014685\n",
      "timestep 76 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150554\n",
      "timestep 77 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146303\n",
      "timestep 78 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147164\n",
      "timestep 79 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147435\n",
      "timestep 80 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165058\n",
      "timestep 81 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172911\n",
      "timestep 82 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172154\n",
      "----Random Action ----\n",
      "timestep 83 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0177187\n",
      "timestep 84 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0191528\n",
      "timestep 85 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183634\n",
      "timestep 86 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170949\n",
      "----Random Action ----\n",
      "timestep 87 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0200495\n",
      "timestep 88 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.018756\n",
      "timestep 89 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0186525\n",
      "timestep 90 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.01763\n",
      "timestep 91 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0191548\n",
      "timestep 92 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0193038\n",
      "timestep 93 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0195021\n",
      "----Random Action ----\n",
      "timestep 94 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166824\n",
      "timestep 95 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0164862\n",
      "timestep 96 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162339\n",
      "timestep 97 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183888\n",
      "----Random Action ----\n",
      "timestep 98 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0196211\n",
      "timestep 99 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0164996\n",
      "timestep 100 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0204656\n",
      "timestep 101 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0171603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 102 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157752\n",
      "timestep 103 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150229\n",
      "----Random Action ----\n",
      "timestep 104 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146109\n",
      "----Random Action ----\n",
      "timestep 105 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145725\n",
      "timestep 106 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156718\n",
      "timestep 107 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156464\n",
      "----Random Action ----\n",
      "timestep 108 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0167699\n",
      "timestep 109 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166109\n",
      "timestep 110 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159744\n",
      "timestep 111 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147566\n",
      "timestep 112 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146384\n",
      "timestep 113 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145559\n",
      "timestep 114 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0161305\n",
      "timestep 115 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016304\n",
      "timestep 116 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0164574\n",
      "timestep 117 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145547\n",
      "timestep 118 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152591\n",
      "timestep 119 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165039\n",
      "timestep 120 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0180536\n",
      "timestep 121 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157858\n",
      "timestep 122 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151843\n",
      "timestep 123 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157659\n",
      "timestep 124 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141019\n",
      "timestep 125 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0134381\n",
      "timestep 126 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.013151\n",
      "timestep 127 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0133074\n",
      "timestep 128 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0127403\n",
      "timestep 129 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.013784\n",
      "timestep 130 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146885\n",
      "timestep 131 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154673\n",
      "timestep 132 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015777\n",
      "timestep 133 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175606\n",
      "timestep 134 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179892\n",
      "timestep 135 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0174432\n",
      "timestep 136 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160617\n",
      "----Random Action ----\n",
      "timestep 137 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0185279\n",
      "timestep 138 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0178273\n",
      "timestep 139 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170226\n",
      "timestep 140 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015279\n",
      "timestep 141 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169826\n",
      "----Random Action ----\n",
      "timestep 142 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0171653\n",
      "timestep 143 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176831\n",
      "----Random Action ----\n",
      "timestep 144 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.015936\n",
      "timestep 145 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168577\n",
      "timestep 146 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176353\n",
      "timestep 147 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173687\n",
      "timestep 148 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0182907\n",
      "timestep 149 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0149135\n",
      "timestep 150 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0219278\n",
      "timestep 151 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172932\n",
      "timestep 152 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158651\n",
      "timestep 153 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147216\n",
      "timestep 154 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0142751\n",
      "timestep 155 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149278\n",
      "timestep 156 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152686\n",
      "timestep 157 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160831\n",
      "timestep 158 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163338\n",
      "timestep 159 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015865\n",
      "timestep 160 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152466\n",
      "timestep 161 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150543\n",
      "timestep 162 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149353\n",
      "timestep 163 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0144782\n",
      "timestep 164 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157263\n",
      "----Random Action ----\n",
      "timestep 165 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158868\n",
      "timestep 166 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159395\n",
      "timestep 167 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153184\n",
      "timestep 168 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153782\n",
      "timestep 169 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014905\n",
      "timestep 170 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173068\n",
      "timestep 171 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154425\n",
      "timestep 172 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153554\n",
      "timestep 173 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148992\n",
      "timestep 174 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152353\n",
      "timestep 175 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145721\n",
      "timestep 176 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151031\n",
      "----Random Action ----\n",
      "timestep 177 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.01454\n",
      "timestep 178 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146376\n",
      "timestep 179 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147968\n",
      "timestep 180 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016541\n",
      "timestep 181 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172911\n",
      "timestep 182 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172154\n",
      "timestep 183 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177187\n",
      "timestep 184 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0191528\n",
      "timestep 185 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183634\n",
      "timestep 186 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170949\n",
      "timestep 187 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0199069\n",
      "timestep 188 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0186338\n",
      "----Random Action ----\n",
      "timestep 189 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0183751\n",
      "timestep 190 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173084\n",
      "timestep 191 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0185232\n",
      "timestep 192 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0190897\n",
      "timestep 193 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0188937\n",
      "----Random Action ----\n",
      "timestep 194 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0161907\n",
      "timestep 195 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167358\n",
      "timestep 196 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0161562\n",
      "timestep 197 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.017283\n",
      "timestep 198 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0190759\n",
      "timestep 199 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0157975\n",
      "timestep 200 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0204547\n",
      "timestep 201 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168208\n",
      "timestep 202 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157311\n",
      "----Random Action ----\n",
      "timestep 203 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 204 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0142996\n",
      "timestep 205 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014728\n",
      "timestep 206 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155023\n",
      "timestep 207 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155016\n",
      "timestep 208 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167631\n",
      "timestep 209 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016031\n",
      "----Random Action ----\n",
      "timestep 210 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0152073\n",
      "timestep 211 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146193\n",
      "timestep 212 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146174\n",
      "----Random Action ----\n",
      "timestep 213 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0142889\n",
      "timestep 214 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163142\n",
      "----Random Action ----\n",
      "timestep 215 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0159958\n",
      "timestep 216 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158225\n",
      "timestep 217 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146874\n",
      "timestep 218 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148495\n",
      "----Random Action ----\n",
      "timestep 219 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0164206\n",
      "timestep 220 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0178011\n",
      "timestep 221 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153197\n",
      "timestep 222 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153787\n",
      "timestep 223 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160192\n",
      "timestep 224 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.01492\n",
      "timestep 225 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0143985\n",
      "timestep 226 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0142118\n",
      "timestep 227 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152604\n",
      "timestep 228 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150846\n",
      "timestep 229 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153511\n",
      "timestep 230 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166366\n",
      "timestep 231 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172664\n",
      "----Random Action ----\n",
      "timestep 232 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169006\n",
      "timestep 233 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179309\n",
      "timestep 234 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0191997\n",
      "timestep 235 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183117\n",
      "timestep 236 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162734\n",
      "timestep 237 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0181888\n",
      "timestep 238 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177744\n",
      "timestep 239 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173259\n",
      "timestep 240 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162721\n",
      "----Random Action ----\n",
      "timestep 241 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0182904\n",
      "timestep 242 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0184563\n",
      "timestep 243 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016592\n",
      "timestep 244 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153023\n",
      "timestep 245 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165245\n",
      "timestep 246 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158951\n",
      "timestep 247 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154337\n",
      "timestep 248 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176961\n",
      "timestep 249 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0146218\n",
      "timestep 250 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0178705\n",
      "timestep 251 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176083\n",
      "timestep 252 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157705\n",
      "timestep 253 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151608\n",
      "timestep 254 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0143657\n",
      "timestep 255 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149922\n",
      "timestep 256 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155151\n",
      "timestep 257 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163471\n",
      "timestep 258 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170131\n",
      "timestep 259 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167302\n",
      "timestep 260 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0161561\n",
      "timestep 261 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151727\n",
      "timestep 262 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150734\n",
      "timestep 263 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158148\n",
      "timestep 264 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166957\n",
      "timestep 265 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175402\n",
      "timestep 266 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0171969\n",
      "timestep 267 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0161893\n",
      "timestep 268 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162988\n",
      "timestep 269 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165854\n",
      "timestep 270 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0189576\n",
      "timestep 271 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0171025\n",
      "timestep 272 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016676\n",
      "timestep 273 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160854\n",
      "timestep 274 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015685\n",
      "timestep 275 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0137901\n",
      "timestep 276 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149143\n",
      "timestep 277 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0139951\n",
      "timestep 278 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0140656\n",
      "timestep 279 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0142141\n",
      "timestep 280 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156273\n",
      "timestep 281 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167941\n",
      "timestep 282 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151443\n",
      "timestep 283 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175403\n",
      "timestep 284 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.018324\n",
      "timestep 285 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.018452\n",
      "timestep 286 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166876\n",
      "timestep 287 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0187598\n",
      "timestep 288 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0171988\n",
      "timestep 289 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165173\n",
      "timestep 290 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0144999\n",
      "timestep 291 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166349\n",
      "timestep 292 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165363\n",
      "timestep 293 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167118\n",
      "timestep 294 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163269\n",
      "timestep 295 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162416\n",
      "timestep 296 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165165\n",
      "timestep 297 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0161012\n",
      "timestep 298 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176474\n",
      "timestep 299 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0169458\n",
      "timestep 300 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0187261\n",
      "timestep 301 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168838\n",
      "timestep 302 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162411\n",
      "timestep 303 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147725\n",
      "----Random Action ----\n",
      "timestep 304 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.014329\n",
      "timestep 305 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149296\n",
      "timestep 306 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Random Action ----\n",
      "timestep 307 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157867\n",
      "timestep 308 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168457\n",
      "timestep 309 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159402\n",
      "timestep 310 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152187\n",
      "timestep 311 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145327\n",
      "timestep 312 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147785\n",
      "timestep 313 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014248\n",
      "timestep 314 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162952\n",
      "timestep 315 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160991\n",
      "timestep 316 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155166\n",
      "timestep 317 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148406\n",
      "timestep 318 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148961\n",
      "timestep 319 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159814\n",
      "timestep 320 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0180207\n",
      "timestep 321 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153806\n",
      "timestep 322 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015114\n",
      "timestep 323 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0161692\n",
      "timestep 324 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150004\n",
      "timestep 325 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0143358\n",
      "timestep 326 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0143385\n",
      "timestep 327 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153395\n",
      "timestep 328 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149651\n",
      "timestep 329 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153808\n",
      "timestep 330 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167216\n",
      "timestep 331 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172664\n",
      "timestep 332 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169006\n",
      "timestep 333 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179309\n",
      "timestep 334 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0191997\n",
      "timestep 335 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183117\n",
      "timestep 336 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162734\n",
      "timestep 337 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183435\n",
      "timestep 338 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177811\n",
      "timestep 339 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0174635\n",
      "timestep 340 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163721\n",
      "timestep 341 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0184472\n",
      "timestep 342 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0184209\n",
      "----Random Action ----\n",
      "timestep 343 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.017829\n",
      "timestep 344 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016343\n",
      "timestep 345 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166499\n",
      "timestep 346 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163578\n",
      "timestep 347 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016171\n",
      "timestep 348 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0185601\n",
      "timestep 349 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0160998\n",
      "timestep 350 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0207089\n",
      "timestep 351 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176388\n",
      "timestep 352 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155543\n",
      "timestep 353 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147725\n",
      "timestep 354 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014329\n",
      "timestep 355 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149296\n",
      "timestep 356 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153663\n",
      "timestep 357 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157867\n",
      "timestep 358 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168457\n",
      "timestep 359 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159402\n",
      "timestep 360 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152187\n",
      "timestep 361 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145327\n",
      "timestep 362 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147785\n",
      "timestep 363 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014248\n",
      "----Random Action ----\n",
      "timestep 364 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0162952\n",
      "timestep 365 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160991\n",
      "timestep 366 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155166\n",
      "timestep 367 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148406\n",
      "timestep 368 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148961\n",
      "timestep 369 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159814\n",
      "timestep 370 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0180207\n",
      "----Random Action ----\n",
      "timestep 371 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0153806\n",
      "----Random Action ----\n",
      "timestep 372 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0152172\n",
      "timestep 373 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0161028\n",
      "timestep 374 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150501\n",
      "timestep 375 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0144045\n",
      "timestep 376 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0143349\n",
      "timestep 377 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154852\n",
      "timestep 378 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149393\n",
      "timestep 379 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152948\n",
      "timestep 380 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016682\n",
      "timestep 381 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172664\n",
      "timestep 382 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169006\n",
      "timestep 383 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179309\n",
      "timestep 384 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0191997\n",
      "timestep 385 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183117\n",
      "timestep 386 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162734\n",
      "timestep 387 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183435\n",
      "timestep 388 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177811\n",
      "timestep 389 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0174635\n",
      "timestep 390 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163721\n",
      "timestep 391 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0184472\n",
      "timestep 392 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0184209\n",
      "timestep 393 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.017829\n",
      "timestep 394 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016343\n",
      "timestep 395 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166499\n",
      "timestep 396 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163578\n",
      "timestep 397 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016171\n",
      "timestep 398 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0185601\n",
      "timestep 399 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0160998\n",
      "timestep 400 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0207089\n",
      "----Random Action ----\n",
      "timestep 401 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175938\n",
      "timestep 402 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152678\n",
      "----Random Action ----\n",
      "timestep 403 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149974\n",
      "timestep 404 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0135234\n",
      "timestep 405 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0143632\n",
      "timestep 406 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156996\n",
      "timestep 407 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159267\n",
      "timestep 408 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165871\n",
      "timestep 409 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 410 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016032\n",
      "timestep 411 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153851\n",
      "timestep 412 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148152\n",
      "----Random Action ----\n",
      "timestep 413 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0149051\n",
      "----Random Action ----\n",
      "timestep 414 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0155099\n",
      "timestep 415 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015879\n",
      "timestep 416 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168581\n",
      "timestep 417 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153158\n",
      "timestep 418 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156036\n",
      "----Random Action ----\n",
      "timestep 419 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0166545\n",
      "timestep 420 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0182817\n",
      "timestep 421 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176579\n",
      "----Random Action ----\n",
      "timestep 422 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0171201\n",
      "----Random Action ----\n",
      "timestep 423 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0173303\n",
      "----Random Action ----\n",
      "timestep 424 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015456\n",
      "timestep 425 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148363\n",
      "timestep 426 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156348\n",
      "timestep 427 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148226\n",
      "timestep 428 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157837\n",
      "timestep 429 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0143247\n",
      "timestep 430 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165275\n",
      "timestep 431 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165531\n",
      "timestep 432 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151096\n",
      "timestep 433 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183536\n",
      "timestep 434 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0184394\n",
      "timestep 435 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176953\n",
      "timestep 436 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170819\n",
      "timestep 437 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0190672\n",
      "timestep 438 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.017065\n",
      "timestep 439 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151496\n",
      "timestep 440 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156138\n",
      "----Random Action ----\n",
      "timestep 441 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0174419\n",
      "timestep 442 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175259\n",
      "----Random Action ----\n",
      "timestep 443 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0184114\n",
      "timestep 444 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173844\n",
      "timestep 445 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169443\n",
      "timestep 446 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165327\n",
      "timestep 447 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0180279\n",
      "timestep 448 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0190909\n",
      "----Random Action ----\n",
      "timestep 449 /state observe /epsilon 0.1 /action 0 /reward -1 /q_max 0.0184101\n",
      "timestep 450 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0207159\n",
      "timestep 451 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175551\n",
      "timestep 452 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016268\n",
      "timestep 453 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147725\n",
      "timestep 454 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014329\n",
      "timestep 455 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149296\n",
      "timestep 456 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153663\n",
      "timestep 457 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157867\n",
      "timestep 458 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168457\n",
      "timestep 459 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159402\n",
      "----Random Action ----\n",
      "timestep 460 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152187\n",
      "timestep 461 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145086\n",
      "timestep 462 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146965\n",
      "timestep 463 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0142889\n",
      "timestep 464 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163142\n",
      "timestep 465 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159958\n",
      "timestep 466 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157888\n",
      "timestep 467 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146591\n",
      "timestep 468 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150034\n",
      "timestep 469 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160835\n",
      "timestep 470 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177981\n",
      "timestep 471 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154075\n",
      "timestep 472 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152781\n",
      "timestep 473 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160307\n",
      "timestep 474 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150263\n",
      "----Random Action ----\n",
      "timestep 475 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0144045\n",
      "timestep 476 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0143349\n",
      "timestep 477 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154852\n",
      "timestep 478 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149393\n",
      "timestep 479 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152948\n",
      "timestep 480 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016682\n",
      "----Random Action ----\n",
      "timestep 481 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172664\n",
      "timestep 482 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169006\n",
      "----Random Action ----\n",
      "timestep 483 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179309\n",
      "timestep 484 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0191997\n",
      "timestep 485 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183117\n",
      "timestep 486 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162734\n",
      "timestep 487 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0182545\n",
      "timestep 488 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177755\n",
      "timestep 489 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172724\n",
      "----Random Action ----\n",
      "timestep 490 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0160952\n",
      "timestep 491 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179841\n",
      "timestep 492 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.018223\n",
      "timestep 493 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016491\n",
      "timestep 494 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154376\n",
      "timestep 495 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015939\n",
      "timestep 496 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156774\n",
      "timestep 497 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149397\n",
      "----Random Action ----\n",
      "timestep 498 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165863\n",
      "timestep 499 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0142133\n",
      "timestep 500 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0187337\n",
      "timestep 501 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167531\n",
      "timestep 502 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156863\n",
      "----Random Action ----\n",
      "timestep 503 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148108\n",
      "timestep 504 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.01424\n",
      "timestep 505 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152014\n",
      "timestep 506 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155003\n",
      "timestep 507 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163511\n",
      "----Random Action ----\n",
      "timestep 508 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 509 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168185\n",
      "timestep 510 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162724\n",
      "timestep 511 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162733\n",
      "timestep 512 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154098\n",
      "timestep 513 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145856\n",
      "timestep 514 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159663\n",
      "timestep 515 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163652\n",
      "timestep 516 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177765\n",
      "timestep 517 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159793\n",
      "timestep 518 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168926\n",
      "----Random Action ----\n",
      "timestep 519 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0169625\n",
      "timestep 520 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.019602\n",
      "timestep 521 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0185668\n",
      "timestep 522 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0187387\n",
      "timestep 523 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183809\n",
      "timestep 524 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175624\n",
      "timestep 525 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156254\n",
      "timestep 526 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159126\n",
      "----Random Action ----\n",
      "timestep 527 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0154462\n",
      "timestep 528 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154293\n",
      "timestep 529 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158675\n",
      "timestep 530 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175886\n",
      "timestep 531 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172145\n",
      "timestep 532 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0174539\n",
      "----Random Action ----\n",
      "timestep 533 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0190465\n",
      "timestep 534 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0193405\n",
      "timestep 535 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0192745\n",
      "timestep 536 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.019013\n",
      "timestep 537 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.020632\n",
      "timestep 538 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179458\n",
      "timestep 539 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166444\n",
      "timestep 540 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168092\n",
      "timestep 541 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175037\n",
      "timestep 542 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169877\n",
      "----Random Action ----\n",
      "timestep 543 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183762\n",
      "timestep 544 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168282\n",
      "timestep 545 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163997\n",
      "timestep 546 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165461\n",
      "timestep 547 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0184115\n",
      "timestep 548 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0193827\n",
      "----Random Action ----\n",
      "timestep 549 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0186752\n",
      "timestep 550 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.02127\n",
      "timestep 551 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177266\n",
      "----Random Action ----\n",
      "timestep 552 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.016309\n",
      "timestep 553 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153179\n",
      "timestep 554 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141792\n",
      "timestep 555 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0144726\n",
      "timestep 556 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157399\n",
      "timestep 557 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160267\n",
      "timestep 558 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169595\n",
      "timestep 559 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167396\n",
      "timestep 560 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160849\n",
      "timestep 561 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152097\n",
      "----Random Action ----\n",
      "timestep 562 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0149732\n",
      "timestep 563 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159509\n",
      "timestep 564 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166934\n",
      "timestep 565 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173454\n",
      "timestep 566 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0174142\n",
      "timestep 567 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158843\n",
      "timestep 568 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0164587\n",
      "timestep 569 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016733\n",
      "timestep 570 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0187353\n",
      "timestep 571 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170448\n",
      "timestep 572 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168252\n",
      "timestep 573 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159846\n",
      "----Random Action ----\n",
      "timestep 574 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0157019\n",
      "timestep 575 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0136745\n",
      "timestep 576 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148233\n",
      "timestep 577 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0142413\n",
      "timestep 578 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0140394\n",
      "timestep 579 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141323\n",
      "timestep 580 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156306\n",
      "timestep 581 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167941\n",
      "timestep 582 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151443\n",
      "timestep 583 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175403\n",
      "----Random Action ----\n",
      "timestep 584 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.018324\n",
      "timestep 585 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.018452\n",
      "timestep 586 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166876\n",
      "timestep 587 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0186756\n",
      "timestep 588 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0171645\n",
      "timestep 589 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165855\n",
      "timestep 590 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147462\n",
      "timestep 591 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169079\n",
      "timestep 592 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170058\n",
      "----Random Action ----\n",
      "timestep 593 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0178068\n",
      "timestep 594 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169945\n",
      "----Random Action ----\n",
      "timestep 595 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169066\n",
      "timestep 596 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166171\n",
      "timestep 597 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176954\n",
      "timestep 598 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0192965\n",
      "timestep 599 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0186371\n",
      "----Random Action ----\n",
      "timestep 600 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0207228\n",
      "timestep 601 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176725\n",
      "timestep 602 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163649\n",
      "timestep 603 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150229\n",
      "timestep 604 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141695\n",
      "timestep 605 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014756\n",
      "timestep 606 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015459\n",
      "----Random Action ----\n",
      "timestep 607 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0152973\n",
      "timestep 608 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165727\n",
      "timestep 609 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165298\n",
      "timestep 610 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153253\n",
      "timestep 611 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145641\n",
      "timestep 612 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 613 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0139901\n",
      "----Random Action ----\n",
      "timestep 614 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015869\n",
      "timestep 615 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152102\n",
      "timestep 616 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155273\n",
      "----Random Action ----\n",
      "timestep 617 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0147595\n",
      "timestep 618 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151716\n",
      "timestep 619 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170433\n",
      "timestep 620 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183273\n",
      "timestep 621 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163552\n",
      "timestep 622 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156443\n",
      "timestep 623 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0164085\n",
      "timestep 624 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148974\n",
      "timestep 625 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0140763\n",
      "timestep 626 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0140329\n",
      "timestep 627 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150251\n",
      "timestep 628 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0139413\n",
      "timestep 629 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0139459\n",
      "timestep 630 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159082\n",
      "timestep 631 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172482\n",
      "timestep 632 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173268\n",
      "timestep 633 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183626\n",
      "timestep 634 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0196574\n",
      "timestep 635 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183385\n",
      "timestep 636 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165614\n",
      "timestep 637 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0189408\n",
      "timestep 638 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0178716\n",
      "timestep 639 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173725\n",
      "----Random Action ----\n",
      "timestep 640 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0162068\n",
      "timestep 641 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0178396\n",
      "----Random Action ----\n",
      "timestep 642 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0182899\n",
      "timestep 643 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0181069\n",
      "----Random Action ----\n",
      "timestep 644 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168817\n",
      "timestep 645 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175373\n",
      "timestep 646 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0181169\n",
      "timestep 647 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0181335\n",
      "----Random Action ----\n",
      "timestep 648 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0196503\n",
      "timestep 649 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0162874\n",
      "timestep 650 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0200178\n",
      "timestep 651 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166744\n",
      "----Random Action ----\n",
      "timestep 652 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0156994\n",
      "timestep 653 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148176\n",
      "timestep 654 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0139463\n",
      "timestep 655 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147073\n",
      "timestep 656 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156\n",
      "timestep 657 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016554\n",
      "timestep 658 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170162\n",
      "timestep 659 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169236\n",
      "----Random Action ----\n",
      "timestep 660 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016246\n",
      "timestep 661 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153309\n",
      "----Random Action ----\n",
      "timestep 662 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147481\n",
      "timestep 663 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014926\n",
      "timestep 664 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155433\n",
      "----Random Action ----\n",
      "timestep 665 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156856\n",
      "timestep 666 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165945\n",
      "timestep 667 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153449\n",
      "timestep 668 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015614\n",
      "----Random Action ----\n",
      "timestep 669 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0163527\n",
      "timestep 670 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0184717\n",
      "timestep 671 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176346\n",
      "timestep 672 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172959\n",
      "timestep 673 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172832\n",
      "timestep 674 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156932\n",
      "timestep 675 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150211\n",
      "timestep 676 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154236\n",
      "timestep 677 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015186\n",
      "timestep 678 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155907\n",
      "timestep 679 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141418\n",
      "timestep 680 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163504\n",
      "timestep 681 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165531\n",
      "timestep 682 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151096\n",
      "timestep 683 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183536\n",
      "timestep 684 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0184394\n",
      "----Random Action ----\n",
      "timestep 685 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0176953\n",
      "timestep 686 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170819\n",
      "timestep 687 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0190807\n",
      "timestep 688 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0171371\n",
      "timestep 689 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152079\n",
      "timestep 690 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156352\n",
      "----Random Action ----\n",
      "timestep 691 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0177995\n",
      "timestep 692 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176289\n",
      "timestep 693 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175621\n",
      "timestep 694 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170628\n",
      "timestep 695 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.01674\n",
      "timestep 696 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163707\n",
      "timestep 697 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165812\n",
      "timestep 698 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0184832\n",
      "timestep 699 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0169451\n",
      "timestep 700 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179966\n",
      "timestep 701 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173376\n",
      "timestep 702 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159566\n",
      "timestep 703 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152616\n",
      "timestep 704 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0142189\n",
      "timestep 705 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146361\n",
      "timestep 706 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150569\n",
      "timestep 707 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016122\n",
      "timestep 708 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167332\n",
      "timestep 709 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0164458\n",
      "timestep 710 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162171\n",
      "timestep 711 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147944\n",
      "timestep 712 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157613\n",
      "timestep 713 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157122\n",
      "timestep 714 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 715 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167307\n",
      "timestep 716 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160904\n",
      "timestep 717 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146351\n",
      "timestep 718 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150272\n",
      "timestep 719 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152227\n",
      "timestep 720 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179325\n",
      "timestep 721 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153899\n",
      "----Random Action ----\n",
      "timestep 722 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147922\n",
      "timestep 723 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150437\n",
      "timestep 724 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0134122\n",
      "timestep 725 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0127283\n",
      "timestep 726 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0139777\n",
      "timestep 727 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0138073\n",
      "timestep 728 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0132016\n",
      "----Random Action ----\n",
      "timestep 729 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0131026\n",
      "timestep 730 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153223\n",
      "timestep 731 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163306\n",
      "timestep 732 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156485\n",
      "----Random Action ----\n",
      "timestep 733 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0181024\n",
      "timestep 734 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.018821\n",
      "timestep 735 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172848\n",
      "timestep 736 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156695\n",
      "timestep 737 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0181108\n",
      "timestep 738 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175019\n",
      "timestep 739 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159949\n",
      "timestep 740 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149963\n",
      "timestep 741 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170405\n",
      "----Random Action ----\n",
      "timestep 742 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173376\n",
      "timestep 743 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.017241\n",
      "timestep 744 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0171\n",
      "timestep 745 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165887\n",
      "timestep 746 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166394\n",
      "timestep 747 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159698\n",
      "timestep 748 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0180198\n",
      "timestep 749 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0162138\n",
      "timestep 750 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0191386\n",
      "timestep 751 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176054\n",
      "timestep 752 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167342\n",
      "timestep 753 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150229\n",
      "timestep 754 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146109\n",
      "timestep 755 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145725\n",
      "timestep 756 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156001\n",
      "----Random Action ----\n",
      "timestep 757 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159663\n",
      "timestep 758 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167041\n",
      "timestep 759 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165745\n",
      "timestep 760 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159759\n",
      "timestep 761 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0147566\n",
      "timestep 762 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146384\n",
      "timestep 763 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145559\n",
      "timestep 764 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0161305\n",
      "timestep 765 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016304\n",
      "timestep 766 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0164574\n",
      "timestep 767 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145547\n",
      "timestep 768 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152591\n",
      "timestep 769 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165039\n",
      "timestep 770 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0180536\n",
      "timestep 771 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157858\n",
      "timestep 772 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151843\n",
      "timestep 773 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157659\n",
      "timestep 774 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141019\n",
      "----Random Action ----\n",
      "timestep 775 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0134381\n",
      "timestep 776 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.013151\n",
      "----Random Action ----\n",
      "timestep 777 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0133074\n",
      "timestep 778 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0127403\n",
      "----Random Action ----\n",
      "timestep 779 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.013784\n",
      "timestep 780 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146885\n",
      "timestep 781 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154673\n",
      "timestep 782 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015777\n",
      "timestep 783 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175606\n",
      "timestep 784 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179892\n",
      "timestep 785 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0174432\n",
      "timestep 786 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160617\n",
      "timestep 787 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0186221\n",
      "timestep 788 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0178619\n",
      "timestep 789 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175568\n",
      "timestep 790 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152857\n",
      "----Random Action ----\n",
      "timestep 791 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0167915\n",
      "timestep 792 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.017169\n",
      "timestep 793 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173713\n",
      "timestep 794 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0161299\n",
      "timestep 795 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163603\n",
      "timestep 796 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177134\n",
      "timestep 797 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168514\n",
      "timestep 798 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0186048\n",
      "timestep 799 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0154414\n",
      "timestep 800 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0209911\n",
      "timestep 801 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170566\n",
      "timestep 802 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156428\n",
      "timestep 803 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150229\n",
      "timestep 804 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141695\n",
      "timestep 805 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014756\n",
      "timestep 806 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015459\n",
      "timestep 807 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152973\n",
      "timestep 808 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165727\n",
      "timestep 809 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165298\n",
      "----Random Action ----\n",
      "timestep 810 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0153253\n",
      "timestep 811 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145641\n",
      "timestep 812 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146119\n",
      "timestep 813 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0139901\n",
      "timestep 814 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 815 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152102\n",
      "timestep 816 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0155273\n",
      "----Random Action ----\n",
      "timestep 817 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0147595\n",
      "timestep 818 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151701\n",
      "timestep 819 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169731\n",
      "timestep 820 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0182998\n",
      "timestep 821 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162676\n",
      "timestep 822 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157817\n",
      "timestep 823 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162771\n",
      "timestep 824 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149685\n",
      "timestep 825 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141133\n",
      "timestep 826 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0139762\n",
      "----Random Action ----\n",
      "timestep 827 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151351\n",
      "timestep 828 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0140123\n",
      "timestep 829 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0138857\n",
      "timestep 830 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158654\n",
      "timestep 831 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172482\n",
      "timestep 832 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173268\n",
      "timestep 833 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183626\n",
      "timestep 834 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0196574\n",
      "timestep 835 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183385\n",
      "timestep 836 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165614\n",
      "timestep 837 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0188218\n",
      "timestep 838 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177435\n",
      "timestep 839 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.017093\n",
      "timestep 840 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163897\n",
      "timestep 841 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0181459\n",
      "timestep 842 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0182091\n",
      "timestep 843 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183065\n",
      "timestep 844 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165522\n",
      "timestep 845 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0180333\n",
      "timestep 846 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175292\n",
      "timestep 847 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0185208\n",
      "timestep 848 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0196134\n",
      "timestep 849 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0153356\n",
      "timestep 850 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0210536\n",
      "timestep 851 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.017029\n",
      "timestep 852 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015888\n",
      "----Random Action ----\n",
      "timestep 853 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0150229\n",
      "timestep 854 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141695\n",
      "timestep 855 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014756\n",
      "timestep 856 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015459\n",
      "timestep 857 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152973\n",
      "----Random Action ----\n",
      "timestep 858 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0165727\n",
      "timestep 859 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165298\n",
      "----Random Action ----\n",
      "timestep 860 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153253\n",
      "timestep 861 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145169\n",
      "timestep 862 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145634\n",
      "timestep 863 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0139758\n",
      "timestep 864 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159535\n",
      "timestep 865 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150946\n",
      "timestep 866 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015729\n",
      "----Random Action ----\n",
      "timestep 867 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0145472\n",
      "timestep 868 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153048\n",
      "timestep 869 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170584\n",
      "timestep 870 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.018189\n",
      "timestep 871 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162676\n",
      "timestep 872 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0157817\n",
      "timestep 873 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162771\n",
      "timestep 874 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0149685\n",
      "timestep 875 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0141133\n",
      "timestep 876 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0139762\n",
      "timestep 877 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151351\n",
      "timestep 878 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0140123\n",
      "timestep 879 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0138857\n",
      "timestep 880 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158654\n",
      "timestep 881 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172482\n",
      "timestep 882 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173268\n",
      "timestep 883 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183626\n",
      "timestep 884 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0196574\n",
      "timestep 885 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183385\n",
      "timestep 886 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165614\n",
      "timestep 887 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0188218\n",
      "timestep 888 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177435\n",
      "timestep 889 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.017093\n",
      "timestep 890 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163897\n",
      "timestep 891 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0181459\n",
      "timestep 892 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0182091\n",
      "----Random Action ----\n",
      "timestep 893 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0183065\n",
      "timestep 894 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165522\n",
      "timestep 895 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0180333\n",
      "timestep 896 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175292\n",
      "timestep 897 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0185208\n",
      "timestep 898 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0196134\n",
      "timestep 899 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0153356\n",
      "timestep 900 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0210536\n",
      "timestep 901 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0170422\n",
      "timestep 902 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0158047\n",
      "timestep 903 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0150229\n",
      "timestep 904 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146109\n",
      "timestep 905 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145725\n",
      "timestep 906 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0156001\n",
      "timestep 907 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159663\n",
      "timestep 908 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167418\n",
      "timestep 909 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165789\n",
      "timestep 910 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160481\n",
      "timestep 911 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146858\n",
      "timestep 912 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.014771\n",
      "----Random Action ----\n",
      "timestep 913 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0144765\n",
      "timestep 914 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0162584\n",
      "----Random Action ----\n",
      "timestep 915 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0164857\n",
      "timestep 916 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163985\n",
      "timestep 917 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145688\n",
      "timestep 918 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0151565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 919 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0167324\n",
      "timestep 920 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179844\n",
      "timestep 921 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159761\n",
      "timestep 922 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153089\n",
      "timestep 923 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015872\n",
      "timestep 924 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0139552\n",
      "timestep 925 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0133654\n",
      "----Random Action ----\n",
      "timestep 926 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0131788\n",
      "timestep 927 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0131564\n",
      "timestep 928 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0127756\n",
      "timestep 929 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.013784\n",
      "timestep 930 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0146885\n",
      "timestep 931 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154673\n",
      "timestep 932 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015777\n",
      "timestep 933 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175606\n",
      "timestep 934 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179892\n",
      "timestep 935 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0174432\n",
      "timestep 936 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0160617\n",
      "timestep 937 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.018606\n",
      "timestep 938 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0179914\n",
      "timestep 939 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0171771\n",
      "timestep 940 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154798\n",
      "timestep 941 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175475\n",
      "timestep 942 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173385\n",
      "timestep 943 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0181261\n",
      "timestep 944 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163397\n",
      "timestep 945 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166131\n",
      "timestep 946 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0178591\n",
      "timestep 947 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0185504\n",
      "timestep 948 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.019158\n",
      "timestep 949 /state observe /epsilon 0.1 /action 1 /reward -1 /q_max 0.0158442\n",
      "timestep 950 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0215613\n",
      "timestep 951 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176621\n",
      "timestep 952 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.01596\n",
      "timestep 953 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0148108\n",
      "timestep 954 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0143092\n",
      "timestep 955 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153275\n",
      "timestep 956 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153968\n",
      "timestep 957 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166393\n",
      "timestep 958 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0171629\n",
      "timestep 959 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016577\n",
      "timestep 960 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0163149\n",
      "timestep 961 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0161139\n",
      "timestep 962 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0152845\n",
      "timestep 963 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0145916\n",
      "timestep 964 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.016218\n",
      "timestep 965 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165567\n",
      "timestep 966 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175065\n",
      "timestep 967 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.01628\n",
      "timestep 968 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168498\n",
      "timestep 969 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0168976\n",
      "timestep 970 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0198302\n",
      "timestep 971 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.018479\n",
      "timestep 972 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.018413\n",
      "timestep 973 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0182457\n",
      "timestep 974 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0175275\n",
      "----Random Action ----\n",
      "timestep 975 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0155392\n",
      "timestep 976 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.015953\n",
      "----Random Action ----\n",
      "timestep 977 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0154759\n",
      "timestep 978 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0153407\n",
      "timestep 979 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0159931\n",
      "timestep 980 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0176044\n",
      "timestep 981 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0172145\n",
      "timestep 982 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0174539\n",
      "timestep 983 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0190465\n",
      "timestep 984 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0193405\n",
      "timestep 985 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0192745\n",
      "timestep 986 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.019013\n",
      "timestep 987 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0208072\n",
      "----Random Action ----\n",
      "timestep 988 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0179262\n",
      "----Random Action ----\n",
      "timestep 989 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0163473\n",
      "timestep 990 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0169042\n",
      "timestep 991 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0178533\n",
      "----Random Action ----\n",
      "timestep 992 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0170632\n",
      "timestep 993 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0177332\n",
      "timestep 994 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0166059\n",
      "timestep 995 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165392\n",
      "timestep 996 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0165169\n",
      "timestep 997 /state observe /epsilon 0.1 /action 1 /reward 0.1 /q_max 0.0173813\n",
      "----Random Action ----\n",
      "timestep 998 /state observe /epsilon 0.1 /action 0 /reward 0.1 /q_max 0.0190735\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2436fc2ab63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-85988996adc9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_fc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_fc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-ab5f1668bc61>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(s, readout, h_fc1, sess)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# run the selected action and observe next state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mx_t1_colored\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mx_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t1_colored\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/flappybird_rewrite/wrapped_flappy_bird.py\u001b[0m in \u001b[0;36mframe_step\u001b[0;34m(self, input_actions)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_surface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mFPSCLOCK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;31m#print self.upperPipes[0]['y'] + PIPE_HEIGHT - int(BASEY * 0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = np.random.randn(2,2,4)\n",
    "print(s)\n",
    "np.shape(s)\n",
    "# np.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x=np.random.rand(2,2,1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = np.append(x, s[:, :,1:], axis=2)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s[:, :, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  \n",
    "slice = random.sample(list, 5)  #从list中随机获取5个元素，作为一个片断返回  \n",
    "print(slice)  \n",
    "# print list #原有序列并没有改变。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
